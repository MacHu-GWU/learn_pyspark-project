{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5c0bdd-0257-435c-8401-dbfd8f78dec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://49ba060e6716:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff84c40370>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ac6ea-b57c-4639-8165-04720b8d3241",
   "metadata": {},
   "source": [
    "## Window rangeBetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "811b8476-0314-4a16-8f1f-7365ac5bee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "| 10|       a|\n",
      "| 20|       a|\n",
      "| 30|       a|\n",
      "| 40|       b|\n",
      "| 50|       b|\n",
      "| 60|       b|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (10, \"a\"),\n",
    "        (20, \"a\"),\n",
    "        (30, \"a\"),\n",
    "        (40, \"b\"),\n",
    "        (50, \"b\"),\n",
    "        (60, \"b\"),\n",
    "    ],\n",
    "    [\"id\", \"category\"],\n",
    ")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7902052e-fe53-4052-9611-3b6e3730f12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|category|sum|\n",
      "+---+--------+---+\n",
      "|  1|       a| 10|\n",
      "|  3|       a|  9|\n",
      "|  6|       a|  6|\n",
      "| 10|       b| 25|\n",
      "| 15|       b| 15|\n",
      "| 21|       b| 21|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = (\n",
    "    Window\n",
    "        # 先根据 category 进行分区, 构成一个个的窗\n",
    "        .partitionBy(\"category\")\n",
    "        # 在每个窗内, 根据 id 进行排序\n",
    "        .orderBy(\"id\")\n",
    "        # 为每一行建立一个小型滑窗, 这个语句的意思是对于每行 (我们称之为主要行) 而言, 根据 orderBy 的排序键 id 有一个值, \n",
    "        # 然后向后扫描所有满足 id 位于主要行的 id 值以及主要行 id 值加上 offset (这里是 25) 之间的行作为滑窗\n",
    "        # 在本例中对于 id10 而言, 所有的 id 在 10 <= x <= 35 之间的行都会被包括进来, 也就是 id10, id20, id30 三行\n",
    "        # 而对于 id 20 而言, 所有的 id 在 20 <= x <= 45 之间的行都会被包括进来, 也就是 id20 和 id30 两行\n",
    "        .rangeBetween(Window.currentRow, 25)\n",
    ")\n",
    "(\n",
    "    df.withColumn(\n",
    "        \"sum\", \n",
    "        # 对于每个小滑窗, 我们对 id 进行求和\n",
    "        F.sum(\"id\").over(window),\n",
    "    )\n",
    "    .sort(\"id\", \"category\")\n",
    "    .show()\n",
    ")\n",
    "\n",
    "\n",
    "# 以下计算结果是这样得来的:\n",
    "#\n",
    "# (10, a, 60) 中的 sum = 60 来自于 id10 + id20 + id30\n",
    "# (20, a, 50) 中的 sum = 50 来自于 id20 + id30\n",
    "# (30, a, 30) 中的 sum = 30 来自于 id30\n",
    "# (40, b, 150) 中的 sum = 150 来自于 id40 + id50 + id60\n",
    "# (50, b, 110) 中的 sum = 110 来自于 id50 + id60\n",
    "# (60, b, 60) 中的 sum = 60 来自于 id60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebc94b-b743-419a-ab6e-29d260530193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4571bd8c-006b-40b0-a412-b4032145abca",
   "metadata": {},
   "source": [
    "## Window rowsBetween\n",
    "\n",
    "``pyspark.sql.Window.rowsBetween`` 是用来为在一个窗口中的每一行建立一个小滑窗, 然后对滑窗内的值进行计算. 举例来说, 我们的数据表有一个 partition key ``category``, 每一个 ``category`` 就是一个 Window. 然后对于 Window 中的每一行, 我们取该行以及它的下一行这两行构成一个小型滑窗 (如果这一行已经是最后一行了, 那么就没有下一行, 滑窗内也就只有一行). 下面我们来看一个例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f97f1d7e-589f-4b71-a287-00a133626a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "| 10|       a|\n",
      "| 20|       a|\n",
      "| 30|       a|\n",
      "| 40|       b|\n",
      "| 50|       b|\n",
      "| 60|       b|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (10, \"a\"),\n",
    "        (20, \"a\"),\n",
    "        (30, \"a\"),\n",
    "        (40, \"b\"),\n",
    "        (50, \"b\"),\n",
    "        (60, \"b\"),\n",
    "    ],\n",
    "    [\"id\", \"category\"],\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90bb5a59-66d1-4eb5-a777-2a3e88105401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|category|sum|\n",
      "+---+--------+---+\n",
      "| 10|       a| 30|\n",
      "| 20|       a| 50|\n",
      "| 30|       a| 30|\n",
      "| 40|       b| 90|\n",
      "| 50|       b|110|\n",
      "| 60|       b| 60|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = (\n",
    "    Window\n",
    "        # 先根据 category 进行分区, 构成一个个的窗\n",
    "        .partitionBy(\"category\")\n",
    "        # 在每个窗内, 根据 id 进行排序\n",
    "        .orderBy(\"id\")\n",
    "        # 为每一行建立一个小型滑窗, 这个小型滑窗由该行以及它的下一行 (offset = 1) 构成\n",
    "        .rowsBetween(Window.currentRow, 1)\n",
    ")\n",
    "\n",
    "(\n",
    "\n",
    "    df.withColumn(\n",
    "        \"sum\", \n",
    "        # 对于每个小滑窗, 我们对 id 进行求和\n",
    "        F.sum(\"id\").over(window),\n",
    "    )\n",
    "    .sort(\"id\", \"category\", \"sum\")\n",
    "    .show()\n",
    ")\n",
    "\n",
    "# 以下计算结果是这样得来的:\n",
    "#\n",
    "# (10, a, 30) 中的 sum = 3 来自于 id10 + id20\n",
    "# (20, a, 50) 中的 sum = 5 来自于 id20 + id30\n",
    "# (30, a, 30) 中的 sum = 3 来自于 id30 + id40 (不存在)\n",
    "# (40, b, 90) 中的 sum = 9 来自于 id40 + id50\n",
    "# (50, b, 110) 中的 sum = 11 来自于 id50 + id60\n",
    "# (60, b, 60) 中的 sum = 6 来自于 id60 + id70 (不存在)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bad3ff-50f8-48d0-b279-98141cfc57e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
