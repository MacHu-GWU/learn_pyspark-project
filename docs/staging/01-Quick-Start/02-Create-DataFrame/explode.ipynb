{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d609f1-5216-4a08-b205-1d8071e30ea6",
   "metadata": {},
   "source": [
    "# Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7460206-7d3e-4874-828b-5d66c77b4de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/jovyan/docs/source/00-New/01-Create-DataFrame\n",
      "Current Python version: 3.10.6\n",
      "Current Python interpreter: /opt/conda/bin/python\n",
      "Current site-packages: ['/opt/conda/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import site\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(f\"Current directory: {cwd}\")\n",
    "print(f\"Current Python version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"Current Python interpreter: {sys.executable}\")\n",
    "print(f\"Current site-packages: {site.getsitepackages()}\")\n",
    "\n",
    "sys.path.append(os.path.join(cwd, \"site-packages\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8aa5fd-2d40-4cf9-85d8-c4df81ac4a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b0013415e2ee:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff57228490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先创建一个 Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083eaaf-9b09-4d8a-9bee-cdc1dae3fa5f",
   "metadata": {},
   "source": [
    "## Create DataFrame from Python List of Tuple\n",
    "\n",
    "```\n",
    "data = [\n",
    "    (v11, v12, ...),\n",
    "    (v21, v22, ...),\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93bd267b-a9a5-4401-8d08-8ee0e2873ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- values: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "+---+---------+\n",
      "| id|   values|\n",
      "+---+---------+\n",
      "|  1|   [1, 2]|\n",
      "|  2|[3, 4, 5]|\n",
      "|  3|   [6, 7]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = spark.createDataFrame(\n",
    "    [\n",
    "        (1, [1, 2]), \n",
    "        (2, [3, 4, 5]), \n",
    "        (3, [6, 7]),\n",
    "    ],\n",
    "    (\"id\", \"values\"),\n",
    ")\n",
    "pdf.printSchema()\n",
    "pdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46e4aec2-0ed3-40ca-979b-c4d938cbbf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  1|    2|\n",
      "|  2|    3|\n",
      "|  2|    4|\n",
      "|  2|    5|\n",
      "|  3|    6|\n",
      "|  3|    7|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "pdf.select(\n",
    "    pdf.id,\n",
    "    F.explode(pdf.values).alias(\"value\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72d335c-42a3-43ab-9b45-7c20dd7f2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "| id|   values|value|\n",
      "+---+---------+-----+\n",
      "|  1|   [1, 2]|    1|\n",
      "|  1|   [1, 2]|    2|\n",
      "|  2|[3, 4, 5]|    3|\n",
      "|  2|[3, 4, 5]|    4|\n",
      "|  2|[3, 4, 5]|    5|\n",
      "|  3|   [6, 7]|    6|\n",
      "|  3|   [6, 7]|    7|\n",
      "+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.withColumn(\n",
    "    \"value\",\n",
    "    F.explode(pdf.values).alias(\"value\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13aca285-19c6-489d-82a0-b611874054c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|alice|\n",
      "|  2|  bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = spark.createDataFrame(\n",
    "    [(1, \"alice\"), (2, \"bob\")],\n",
    "    schema=(\"id\", \"name\"),\n",
    ")\n",
    "pdf.printSchema()\n",
    "pdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d178e-64e3-434c-b14f-9d298c6068e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6985ebb9-4204-4593-b74a-bf302a3f9c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c1: long (nullable = true)\n",
      " |-- c2: string (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "+---+---+\n",
      "\n",
      "root\n",
      " |-- c11: long (nullable = true)\n",
      " |-- c22: string (nullable = true)\n",
      " |-- c33: string (nullable = true)\n",
      "\n",
      "+---+---+-----+\n",
      "|c11|c22|  c33|\n",
      "+---+---+-----+\n",
      "|  1|  a|alice|\n",
      "|  1|  a|  bob|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf1 = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"a\"), \n",
    "    ],\n",
    "    schema=(\"c1\", \"c2\"),\n",
    ")\n",
    "pdf1.printSchema()\n",
    "pdf1.show()\n",
    "\n",
    "pdf2 = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"a\", \"alice\"), \n",
    "        (1, \"a\", \"bob\"),\n",
    "    ],\n",
    "    schema=(\"c11\", \"c22\", \"c33\"),\n",
    ")\n",
    "pdf2.printSchema()\n",
    "pdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9262546a-60eb-4925-845d-ae9202a9d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+\n",
      "| c1| c2|c11|c22|  c33|\n",
      "+---+---+---+---+-----+\n",
      "|  1|  a|  1|  a|  bob|\n",
      "|  1|  a|  1|  a|alice|\n",
      "+---+---+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = pdf1.join(\n",
    "    pdf2,\n",
    "    (pdf1.c1 == pdf2.c11) & (pdf1.c2 == pdf2.c22),\n",
    "    \"left\",\n",
    ")\n",
    "pdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d947e-5200-4198-ad91-0b6ace0dd3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fce4400f-aedb-4e7f-8f79-83f09151d657",
   "metadata": {},
   "source": [
    "## Create DataFrame from Python List of Dict\n",
    "\n",
    "```\n",
    "data = [\n",
    "    {\"col1\": v11, \"col2\": v12, ...},\n",
    "    {\"col1\": v21, \"col2\": v22, ...},\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38e5a3b1-160b-49f6-9b41-76182746b734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|alice|\n",
      "|  2|  bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = spark.createDataFrame(\n",
    "    [\n",
    "        {\"id\": 1, \"name\": \"alice\"},\n",
    "        {\"id\": 2, \"name\": \"bob\"},\n",
    "    ],\n",
    ")\n",
    "pdf.printSchema()\n",
    "pdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac41c7-77ef-4635-b345-8579bf43d98a",
   "metadata": {},
   "source": [
    "## Explicitly Define the Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f394d72-6443-467a-ae23-cc27de074b54",
   "metadata": {},
   "source": [
    "创建 DataFrame 的时候, Spark 支持自动推导类型, 也支持显式指定 Schema. [pyspark.sql.types](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html) 这个模块定义了 Spark 中的所有数据类型. 一个 Schema 本质就是一个 Struct 结构体 (key value pair), 里面可以 embed 其他的结构体以描述一个 column 是一个复杂对象的情况. 里面也可以 embed 一个 Array, 其中 Array 里面的元素的类型也需要指定. 用 Struct + Array + generic type 基本上就可以构建出任意复杂的 Schema.\n",
    "\n",
    "下面我们来手动定义一个 Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75154b97-b347-484f-8968-a6013100a72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------------------------+---------+-----------------------+\n",
      "|id |name |profile                    |numbers  |tags                   |\n",
      "+---+-----+---------------------------+---------+-----------------------+\n",
      "|1  |Alice|{111-222-333, [alice, ali]}|[1, 2, 3]|[{profession, banker}] |\n",
      "|2  |Bob  |{444-555-666, [bob, bb]}   |[4, 5, 6]|[{profession, teacher}]|\n",
      "+---+-----+---------------------------+---------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 先 import 需要的类\n",
    "from pyspark.sql.types import ( \n",
    "    IntegerType,\n",
    "    StringType, \n",
    "    DoubleType,    \n",
    "    BooleanType,\n",
    "    TimestampType, # datetime.datetime\n",
    "    DateType, # datetime.date\n",
    "    ArrayType, \n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "\n",
    "# spark session 的 createDataFrame 是通过直接输入数据创建 DataFrame 的方法. 在生产环境下我们一般都是从数据源读取, \n",
    "# 而在开发过程中我们经常要实验性地创建一个 DataFrame 来尝试一些 API. 所以这种方法还是非常有必要掌握的.\n",
    "pdf = spark.createDataFrame(\n",
    "    # 一个列表中的每一个元素代表一个 Row, 你可以用 tuple 或是 dict 来代表一个 Row, 这里用 tuple 为例\n",
    "    # 在使用 tuple 的时候请注意每个值的顺序\n",
    "    [\n",
    "        (\n",
    "            1, \n",
    "            \"Alice\", \n",
    "            {\"ssn\": \"111-222-333\", \"aliases\": [\"alice\", \"ali\"]},\n",
    "            [1, 2, 3],\n",
    "            [\n",
    "                {\"key\": \"profession\", \"value\": \"banker\"},\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            2, \n",
    "            \"Bob\", \n",
    "            {\"ssn\": \"444-555-666\", \"aliases\": [\"bob\", \"bb\"]},\n",
    "            [4, 5, 6],\n",
    "            [\n",
    "                {\"key\": \"profession\", \"value\": \"teacher\"},\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    # 创建 DataFrame 时显式指定 schema 结构体\n",
    "    schema=StructType([\n",
    "        StructField(\"id\", IntegerType(), nullable=True),\n",
    "        StructField(\"name\", StringType(), nullable=True),\n",
    "        StructField(\n",
    "            \"profile\", \n",
    "            StructType([\n",
    "                StructField(\"ssn\", StringType(), nullable=True),\n",
    "                StructField(\"aliases\", ArrayType(StringType()), nullable=True),\n",
    "            ]), \n",
    "            nullable=True,\n",
    "        ),\n",
    "        StructField(\"numbers\", ArrayType(IntegerType()), nullable=True),\n",
    "        StructField(\n",
    "            \"tags\", \n",
    "            ArrayType(StructType([\n",
    "                StructField(\"key\", StringType(), nullable=False),\n",
    "                StructField(\"value\", StringType(), nullable=False),\n",
    "            ])), \n",
    "            nullable=True,\n",
    "        ),\n",
    "    ])\n",
    ")\n",
    "# 用 .show 函数打印数据, 用 vertical = True 来进行列式打印, 用  truncate = False 来显示全部字符串\n",
    "pdf.show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3af25ea7-469f-4565-a18d-f69df93b29c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----\n",
      " id   | 1     \n",
      " name | Alice \n",
      "-RECORD 1-----\n",
      " id   | 2     \n",
      " name | Bob   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.select(\n",
    "    pdf.id,\n",
    "    pdf.name,\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59056c71-7a00-4a64-9faf-d52eed96821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " profile | {111-222-333, [alice, ali]} \n",
      "-RECORD 1------------------------------\n",
      " profile | {444-555-666, [bob, bb]}    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.select(\n",
    "    pdf.profile,\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78fa2cc8-3caa-4a63-ae04-8152e7869501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " profile.ssn | 111-222-333 \n",
      "-RECORD 1------------------\n",
      " profile.ssn | 444-555-666 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.select(\n",
    "    pdf.profile.ssn\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04e20e34-578a-4cbd-8ca2-42bf82ecd261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------\n",
      " profile.aliases | [alice, ali] \n",
      "-RECORD 1-----------------------\n",
      " profile.aliases | [bob, bb]    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.select(\n",
    "    pdf.profile.aliases\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "951725cb-0b27-4fec-8735-ec3945f9dfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------\n",
      " numbers | [1, 2, 3] \n",
      "-RECORD 1------------\n",
      " numbers | [4, 5, 6] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.select(\n",
    "    pdf.numbers\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24e576eb-5202-47ed-9aee-5daffbe0fffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------\n",
      " number0 | 1   \n",
      "-RECORD 1------\n",
      " number0 | 4   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.select(\n",
    "    pdf.numbers[0].alias(\"number0\")\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fd0193a-6fe3-4c54-a3b1-fcd9800fa2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------\n",
      " tags | [{profession, banker}]  \n",
      "-RECORD 1-----------------------\n",
      " tags | [{profession, teacher}] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.select(\n",
    "    pdf.tags\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c8f867a-1900-43b0-aadf-96947c93e01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " tags0.key | profession \n",
      "-RECORD 1---------------\n",
      " tags0.key | profession \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.select(\n",
    "    pdf.tags[0].key.alias(\"tags0.key\")\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a8d34-a40e-4f36-895b-50d15d60337a",
   "metadata": {},
   "source": [
    "## Read CSV\n",
    "\n",
    "\n",
    "``SparkSession.read.csv`` 是用来从 CSV 读数据的函数.\n",
    "\n",
    "Ref: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3914b018-a364-4243-bf37-b9602f546b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|alice|\n",
      "|  2|  bob|\n",
      "|  3|cathy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 默认情况下是 spark 会把第一行当成数据而不是 header, 如果你的 csv 文件有 header 则需要显示指定\n",
    "pdf = spark.read.csv(\"./users.csv\", header=True, sep=\",\")\n",
    "pdf.printSchema()\n",
    "pdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7892eafb-c409-42d7-b4ea-20ca98e5d551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|alice|\n",
      "|  2|  bob|\n",
      "|  3|cathy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 因为 CSV 文件是一个字符串编码的文件, 在读数据的经常会遇到到底数字是被解读为整数还是字符串的问题\n",
    "# 你可以在读取 CSV 文件时定义 Schema, 显式地告诉 Spark 哪些 column 要被视为整数, 哪些 column 要被视为字符串\n",
    "pdf = spark.read.csv(\n",
    "    \"./users.csv\", \n",
    "    header=True,\n",
    "    sep=\",\",\n",
    "    schema=StructType([\n",
    "        StructField(\"id\", IntegerType(), nullable=True),\n",
    "        StructField(\"name\", StringType(), nullable=True),\n",
    "    ]),\n",
    ")\n",
    "pdf.printSchema()\n",
    "pdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34d157d9-8f85-45d7-95af-d9853559ce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|alice|\n",
      "|  2|  bob|\n",
      "|  3|cathy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Python 中经常会遇到从 csv 数据字符串本身读取而不是将其视为一个路径的情况, 在 pandas 等框架中我们一般用 io buffer 来实现\n",
    "# 而在 spark 中你需要将字符串分割为许多行, 其中每行代表一条数据\n",
    "# 然后用 spark.sparkContext.parallelize(lines) 生成一个并行对象 (spark 是分布式并行框架)\n",
    "# 然后就可以用 spark.read.csv 来读数据了\n",
    "\n",
    "import io\n",
    "\n",
    "with open(\"./users.csv\", \"r\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "    # df = spark.read.csv(dt, header=True)\n",
    "    pdf = spark.read.csv(spark.sparkContext.parallelize(lines), header=True, sep=\",\")\n",
    "    pdf.printSchema()\n",
    "    pdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee25bc-aef6-4d8a-83a2-ef3a6c86b36d",
   "metadata": {},
   "source": [
    "## Select A Column\n",
    "\n",
    "对 DataFrame 进行向量化的列操作是大数据处理中最简单的操作之一. 无论多么复杂的操作都是基于此, 要么对一个列的计算过程更复杂, 要么涉及到多个列的计算. 本节我们来看看如何选取一个 Column 并对其进行计算."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b5348d0b-0fd7-494f-b8e5-eed71c001dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|alice|\n",
      "|  2|  bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = spark.createDataFrame(\n",
    "    [(1, \"alice\"), (2, \"bob\")],\n",
    "    (\"id\", \"name\")\n",
    ")\n",
    "pdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09b6722b-339f-4ae6-bc70-e057960ccbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "pdf.select(\n",
    "    F.col(\"id\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8f50222-4ab7-4572-82df-2d5f3ed17a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|name_substring|\n",
      "+--------------+\n",
      "|           lic|\n",
      "|             o|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf.select(\n",
    "    # 去掉第一个字符和最后一个字符\n",
    "    F.expr(\"substring(name, 2, length(name)-2)\").alias(\"name_substring\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f892a9cc-eff6-4342-b394-18ac160c05aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.udf(IntegerType())\n",
    "def double_the_integer(v: int) -> int:\n",
    "    return v * 2\n",
    "\n",
    "pdf.select(\n",
    "    double_the_integer(pdf.id).alias(\"id\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0b733-14a1-4709-9ef3-57526762afcd",
   "metadata": {},
   "source": [
    "## Next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
